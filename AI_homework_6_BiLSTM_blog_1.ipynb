{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_homework_6_BiLSTM_blog_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlk7ae99hRvdmvU4bNM5bq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EricCallaway/COSC_5313_homework_6/blob/main/AI_homework_6_BiLSTM_blog_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5n50XKoigE2r"
      },
      "outputs": [],
      "source": [
        "\"\"\"Sample code for recurrent layer based models\n",
        "\"\"\"\n",
        "\n",
        "# ---- Imports ----\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding\n",
        "\n",
        "# ---- Data loading ----\n",
        "# contains a list or series of sentences\n",
        "# train = ...\n",
        "# test =  ...\n",
        "# combined = train['text'] + test['text']\n",
        "\n",
        "# ---- Data processing ----\n",
        "# set max vocab size\n",
        "vocab = 10000\n",
        "# create tokenizer instances  \n",
        "tokenizer = Tokenizer(num_words=vocab, oov_token=0)\n",
        "# fit the tokenizer on text sequences\n",
        "tokenizer.fit_on_texts(combined)\n",
        "# tokenize the complete data\n",
        "sequence_combined = tokenizer.texts_to_sequences(combined)\n",
        "# get the max len\n",
        "max_len = max([len(x) for x in sequence_combined])\n",
        "# tokenize the train data\n",
        "train_sequences = tokenizer.texts_to_sequences(train['text'])\n",
        "# add padding to the data\n",
        "padded_train_sequence = pad_sequences(train_sequences, maxlen=max_len, dtype='int32', padding='pre', truncating='pre', value=0)\n",
        "\n",
        "# ---- Model ----\n",
        "model = Sequential()\n",
        "# encoder\n",
        "model.add(keras.Input(shape=(padded_train_sequence.shape[1], ))) # input layer\n",
        "model.add(Embedding(vocab, 256)) # embedding layer\n",
        "model.add(Bidirectional(LSTM(256))) # biLSTM layer\n",
        "# decoder\n",
        "model.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "# summary\n",
        "model.summary()\n",
        "\n",
        "# ---- Compile and Train ----\n",
        "# callbacks\n",
        "earlystopping = keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
        "# compile\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
        "# fit\n",
        "history = model.fit(padded_train_sequence, train['y'], epochs=100, batch_size=16, verbose=2, callbacks=[earlystopping])\n",
        "\n",
        "# ---- Prediction ----\n",
        "# prepare testing data\n",
        "test_sequences = tokenizer.texts_to_sequences(test['text'])\n",
        "test_padded_sequences = pad_sequences(test_sequences, maxlen=max_len, dtype='int32', padding='pre',truncating='pre', value=0)\n",
        "# perform prediction\n",
        "y_pred = model.predict(test_padded_sequences)\n"
      ]
    }
  ]
}